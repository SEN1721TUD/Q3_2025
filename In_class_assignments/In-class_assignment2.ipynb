{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEN1721 Travel Behaviour Research\n",
    "\n",
    "## `In-class assignment 2:`\n",
    "## `Latent classs models`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q3 2025**<br>\n",
    "**Instructor:** Sander van Cranenburgh<br>\n",
    "**TA:**  Gabriel Nova <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**In-class assignments aim to:**<br>\n",
    "* Illustrate how models and theory discussed in the classroom work out in practice.\n",
    "* Help you gather hands-on modelling and data analysis skills.\n",
    "\n",
    "**In-class assignments are:**<br>\n",
    "* Learning environments where you work with Python and get support from TA and fellow students.\n",
    "* Not graded and do not have to be submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Use of AI tools`\n",
    "AI tools, such as ChatGPT and Copilot, are great tools to assist with programming. Moreover, in your later careers, you will work in a world where such tools are widely available. As such, we **encourage** you to use AI tools **effectively**. However, be careful not to overestimate the capacity of AI tools! AI tools cannot replace you: you still have to conceptualise the problem, dissect it and structure it to conduct proper analysis. We recommend being especially **reticent** with using AI tools for the more conceptual and reflection-oriented questions. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Learning objectives In-class assignment 2`**\n",
    "\n",
    "After completing the in-class assignment, you will be able to:\n",
    "1. Use statistical criteria to determine the number of classes in a Latent Class model\n",
    "1. Parameterise the class membership function with covariates\n",
    "1. Handle local maxima in the estimation of Latent Class models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Import packages`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will import all the Python libraries that we will use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biogeme\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "from biogeme.expressions import Beta, PanelLikelihoodTrajectory\n",
    "\n",
    "# Import custom estimation functions for Biogeme\n",
    "from bio_estimation_fcns import estimate_LC, print_results\n",
    "\n",
    "# General python packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "# Random number generator\n",
    "from random import random as rand\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Pandas setting to show all columns when displaying a pandas dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1. Load the data set` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create that path to the data file\n",
    "data_path =  Path(f'data/Route_choice_data.csv')\n",
    "\n",
    "# Load the data as a pandas dataframe\n",
    "df = pd.read_csv(data_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of variables**<br>\n",
    "\n",
    "The data contain 3,510 stated route choices from 388 respondent. Each alternative is defined by four attributes: TT, CONG, VAR, and TC.\n",
    "Below is a description of the variables in the data set. For more information about the data, you can have a look at the original paper by [Chorus et al. (2012)](https://link.springer.com/article/10.1007/s11116-012-9444-3). \n",
    "\n",
    "\n",
    "| Variable | Description                                                       | Data type                 |\n",
    "|----------|-------------------------------------------------------------------|---------------------------|\n",
    "| Survey   | Identifier for the survey                                         | Integer                   |\n",
    "| ID       | Unique identifier for each respondent                             | Integer                   |\n",
    "| Quest    | Order of the choice tasks per respondent                          | Integer                   |\n",
    "| CHOICE   | Chosen alternative                                                | Integer                   |\n",
    "| TT1      | Travel Time for Alternative 1                                     | Integer                   |\n",
    "| CONG1    | Percentage of travel time in congestion for Alternative 1         | Integer                   |\n",
    "| VAR1     | Travel time variability for Alternative 1                         | Integer                   |\n",
    "| TC1      | Travel Cost for Alternative 1                                     | Float                     |\n",
    "| TT2      | Travel Time for Alternative 2                                     | Integer                   |\n",
    "| CONG2    | Percentage of travel time in congestion for Alternative 2         | Integer                   |\n",
    "| VAR2     | Travel time variability for Alternative 2                         | Integer                   |\n",
    "| TC2      | Travel Cost for Alternative 2                                     | Float                     |\n",
    "| TT3      | Travel Time for Alternative 3                                     | Integer                   |\n",
    "| CONG3    | Percentage of travel time in congestion for Alternative 3         | Integer                   |\n",
    "| VAR3     | Travel time variability for Alternative 3                         | Integer                   |\n",
    "| TC3      | Travel Cost for Alternative 3                                     | Float                     |\n",
    "| AV1      | Availability of Alternative 1 (0: not available, 1: available)    | Integer                   |\n",
    "| AV2      | Availability of Alternative 2 (0: not available, 1: available)    | Integer                   |\n",
    "| AV3      | Availability of Alternative 3 (0: not available, 1: available)    | Integer                   |\n",
    "| age      | Age of the respondent                                             | Integer                   |\n",
    "| edu      | Educational level                                                 | Integer                   |\n",
    "| edufin   | Max education level                                               | Integer                   |\n",
    "| sex      | Gender of the respondent (e.g., male, female)                     | Integer                   |\n",
    "| E1       | I considered it difficult to make choices across the presented alternatives in this choice experiment | Integer                   |\n",
    "| E2       | I considered it important to make the ‘right’ choice across the presented alternatives                | Integer                   |\n",
    "| E3       | When considering a new job or residential location, having a pleasant commute takes a central place in my decision | Integer      |\n",
    "| E4       | In general, I think making choices is difficult                                                       | Integer                   |\n",
    "| E5       | When decisions are important I consider making choices difficult                                      | Integer                   |\n",
    "| E6       | When making choices, I consider the possibility that I will regret my choice in hindsight             | Integer                   |\n",
    "| E7       | When making choices, I prefer a choice set that is as large as possible                               | Integer                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we did in in-class assignment 1, we remove respondents who did more than 9 choice tasks\n",
    "df = df[df['ID'].map(df['ID'].value_counts()) <= 9]\n",
    "\n",
    "# Create variable T for the number of choice tasks per individual\n",
    "T = df['ID'].value_counts().unique()[0]\n",
    "print(f\"Number of choice tasks per individual: {T}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2. Creating Biogeme database and variables` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Database for panel models`** \n",
    "\n",
    "First, we create the biogeme database which takes into account the panel structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "biodata_panel = db.Database('data_panel', df) # Creates a biogeme database\n",
    "biodata_panel.panel(\"ID\")                     # Specify that the panel structure is defined by \"ID\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Biogeme variables`**<br>\n",
    "Then, we make the variables in our data set globally available so we can use them in the estimation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create biogeme variables\n",
    "for c in biodata_panel.data.columns:\n",
    "    if biodata_panel.data[c].dtypes != 'object': # This excludes the 'object' type columns\n",
    "        globals()[c] = biodata_panel.variables[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3. Determining the optimal number of classes` <br>\n",
    "\n",
    "Determining the number of classes in a latent class model often is a challenging task. Typically, it involves a **trade-off** between statistical performance (BIC) and behavioural interpretebility.<br><br>\n",
    "To determine the optimal number of classes, we create a `for loop` to swiftly estimate LC models with an increasing numbers of classes.<br>\n",
    "In the loop, we store the BIC, LL, and the estimation result object in lists for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store the BIC, LL, and results object\n",
    "BIC = []\n",
    "LL = []\n",
    "lst_LC_results = []\n",
    "\n",
    "# Set the number of classes to estimate\n",
    "lst_classes = [2,3,4,5,6]\n",
    "\n",
    "# Loop over the number of classes to estimate\n",
    "for n_classes in lst_classes:\n",
    "    print(f'Estimating LC model with {n_classes} classes ...')\n",
    "\n",
    "    # Give a name to the model\n",
    "    model_name = f'LC with {n_classes} classes'\n",
    "\n",
    "    # Define the model parameters for each class\n",
    "    B_tt = {}\n",
    "    B_cong = {}\n",
    "    B_var = {}\n",
    "    B_tc = {}\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        B_tt[k]    = Beta(f'B_tt_{k}',   0, None, None, 0)\n",
    "        B_cong[k]  = Beta(f'B_cong_{k}', 0, None, None, 0)\n",
    "        B_var[k]   = Beta(f'B_var_{k}',  0, None, None, 0)\n",
    "        B_tc[k]    = Beta(f'B_tc_{k}',   0, None, None, 0)\n",
    "\n",
    "    # Define the membership model parameters\n",
    "    delta = {}\n",
    "    for k in range(n_classes):  \n",
    "        if k == 0: # The first class (0) is fixed to 0; \n",
    "            delta[k] = Beta(f'delta_{k}', 0, None, None, 1)\n",
    "        else: # We use different random starting values for each class\n",
    "            delta[k] = Beta(f'delta_{k}', rand(), None, None, 0)\n",
    "\n",
    "    # Define the utility functions for each class\n",
    "    V = [{} for _ in range(n_classes)]  # List of dictionaries for each class\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        V[k] = {\n",
    "            1: B_tt[k] * TT1 + B_cong[k] * CONG1 + B_var[k] * VAR1 + B_tc[k] * TC1,\n",
    "            2: B_tt[k] * TT2 + B_cong[k] * CONG2 + B_var[k] * VAR2 + B_tc[k] * TC2,\n",
    "            3: B_tt[k] * TT3 + B_cong[k] * CONG3 + B_var[k] * VAR3 + B_tc[k] * TC3\n",
    "                }\n",
    "\n",
    "    # Availability dictionary (same for all classes)\n",
    "    AV = {1: AV1, 2: AV2, 3: AV3}  \n",
    "\n",
    "    # Define the membership model value functions for each class\n",
    "    nu = [delta[k] for k in range(n_classes)]\n",
    "\n",
    "    # Estimate the LC model\n",
    "    results_LC = estimate_LC(V,AV,nu,CHOICE,biodata_panel,model_name)\n",
    "\n",
    "    # Store the BIC, LL values, and estimation result object in lists\n",
    "    BIC.append(results_LC.get_general_statistics()['Bayesian Information Criterion'][0])\n",
    "    LL.append(results_LC.get_general_statistics()['Final log likelihood'][0])\n",
    "    lst_LC_results.append(results_LC)\n",
    "\n",
    "# Create a pandas dataframe with the results (LL and BIC) for each class\n",
    "df_results = pd.DataFrame({'classes': lst_classes, 'LL':LL,'BIC': BIC}).round(2)\n",
    "\n",
    "# Show the results\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the BIC values as a function of the number of classes\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.lineplot(x='classes', y='BIC', data=df_results,linestyle='dashed', marker='o')\n",
    "plt.xticks(df_results['classes'].unique()) # Force x-axis ticks to be integers\n",
    "plt.xlabel('Number of classes')\n",
    "plt.ylabel('BIC')\n",
    "plt.title('BIC values for different number of classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the estimation results**\n",
    "\n",
    "        --> The plot shows that the statistically best model (i.e. with the minimum BIC) is the model with 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the index (row) with the minimum BIC value (i.e. the best statistical model)\n",
    "idx = df_results.idxmin()['BIC'] \n",
    "\n",
    "# Print the results of the statistically best model\n",
    "print_results(lst_LC_results[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the estimation results**\n",
    "\n",
    "        --> Interpreting the 5 class model is challenging. The classes are not well-separated. Moreover, the many parameters of the model are not statistically significant.\n",
    "\n",
    "        --> Arguably, the 3-class model is the best model, considering behavioural interpretability and statistical performance. This model is more interpretable than the 5-class model. The classes are better separated, and most the parameters are statistically significant. Going from 2 to 3 classes does not improve the BIC much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `4. Parameterisation of the class membership function using covariates` <br>\n",
    "\n",
    "After determining the number of classes, we can parameterise the class membership function using covariates.<br> \n",
    "In this case, we try whether respondent's education helps to explain the class membership (thus their tastes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a name to the model    \n",
    "model_name = 'LC with 3 classes with education as covariate'\n",
    "\n",
    "# Define the model parameters for class 0\n",
    "B_tt_0    = Beta('B_tt_0',  0, None, None, 0)\n",
    "B_cong_0  = Beta('B_cong_0',0, None, None, 0)\n",
    "B_var_0   = Beta('B_var_0', 0, None, None, 0)\n",
    "B_tc_0    = Beta('B_tc_0',  0, None, None, 0)\n",
    "\n",
    "# Define the model parameters for class 1\n",
    "B_tt_1    = Beta('B_tt_1',  0, None, None, 0)\n",
    "B_cong_1  = Beta('B_cong_1',0, None, None, 0)\n",
    "B_var_1   = Beta('B_var_1', 0, None, None, 0)\n",
    "B_tc_1    = Beta('B_tc_1',  0, None, None, 0)\n",
    "\n",
    "# Define the model parameters for class 2\n",
    "B_tt_2    = Beta('B_tt_2',  0, None, None, 0)\n",
    "B_cong_2  = Beta('B_cong_2',0, None, None, 0)\n",
    "B_var_2   = Beta('B_var_2', 0, None, None, 0)\n",
    "B_tc_2    = Beta('B_tc_2',  0, None, None, 0)\n",
    "\n",
    "# Define the membership model parameters\n",
    "delta_0  = Beta('delta_0', 0   , None, None, 1)\n",
    "delta_1  = Beta('delta_1', 0.10, None, None, 0)\n",
    "delta_2  = Beta('delta_2',-0.10, None, None, 0)\n",
    "gamma_edu_1 = Beta('gamma_edu_1', 0, None, None, 0)\n",
    "gamma_edu_2 = Beta('gamma_edu_2', 0, None, None, 0)\n",
    "\n",
    "# Define the utility functions for class 0\n",
    "V1_0 = B_tt_0 * TT1 + B_cong_0 * CONG1 + B_var_0 * VAR1 + B_tc_0 * TC1\n",
    "V2_0 = B_tt_0 * TT2 + B_cong_0 * CONG2 + B_var_0 * VAR2 + B_tc_0 * TC2\n",
    "V3_0 = B_tt_0 * TT3 + B_cong_0 * CONG3 + B_var_0 * VAR3 + B_tc_0 * TC3\n",
    "\n",
    "# Define the utility functions for class 1\n",
    "V1_1 = B_tt_1 * TT1 + B_cong_1 * CONG1 + B_var_1 * VAR1 + B_tc_1 * TC1\n",
    "V2_1 = B_tt_1 * TT2 + B_cong_1 * CONG2 + B_var_1 * VAR2 + B_tc_1 * TC2\n",
    "V3_1 = B_tt_1 * TT3 + B_cong_1 * CONG3 + B_var_1 * VAR3 + B_tc_1 * TC3\n",
    "\n",
    "# Define the utility functions for class 2\n",
    "V1_2 = B_tt_2 * TT1 + B_cong_2 * CONG1 + B_var_2 * VAR1 + B_tc_2 * TC1\n",
    "V2_2 = B_tt_2 * TT2 + B_cong_2 * CONG2 + B_var_2 * VAR2 + B_tc_2 * TC2\n",
    "V3_2 = B_tt_2 * TT3 + B_cong_2 * CONG3 + B_var_2 * VAR3 + B_tc_2 * TC3\n",
    "\n",
    "# Associate utility functions with the numbering of alternatives in the \"choice\" column\n",
    "V_0 = {1: V1_0, 2: V2_0, 3: V3_0}\n",
    "V_1 = {1: V1_1, 2: V2_1, 3: V3_1}\n",
    "V_2 = {1: V1_2, 2: V2_2, 3: V3_2}\n",
    "\n",
    "# Put the dictionary of utility functions in a list\n",
    "V = [V_0, V_1, V_2]\n",
    "\n",
    "# Create a dictionary to describe the availability conditions of each alternative, where 1 indicates that the alternative is available, and 0 indicates that the alternative is not available.\n",
    "AV = {1: AV1, 2: AV2, 3: AV3} \n",
    "\n",
    "# Define the membership model value functions for each class\n",
    "# The term \"PanelLikelihoodTrajectory(edu)**(1/T)\" is a bit strange. But, it is the \"trick\"\" to include the covariate in the membership function.\n",
    "# It first multiplies the covariate over all rows in the database for each individual: PanelLikelihoodTrajectory(edu). Then, it undoes the multiplication by taking it to the power of (1/T).\n",
    "# The result is a the covariate for each individual at the panel level (as opposed to the observation level).\n",
    "nu_0 = delta_0 # Note: one class needs to be fixed to 0. delta_0 is fixed to 0\n",
    "nu_1 = delta_1 + gamma_edu_1 * PanelLikelihoodTrajectory(edu)**(1/T)\n",
    "nu_2 = delta_2 + gamma_edu_2 * PanelLikelihoodTrajectory(edu)**(1/T)\n",
    "\n",
    "# Put membership functions in a list\n",
    "nu = [nu_0, nu_1, nu_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the LC model\n",
    "results_LC_edu = estimate_LC(V,AV,nu,CHOICE,biodata_panel,model_name)\n",
    "\n",
    "# Print the results\n",
    "print_results(results_LC_edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the estimation results**\n",
    "\n",
    "        --> The estimation results show that the education level of the respondent is a significant predictor of the class membership. The higher the education level, the less likely the respondent is to belong to class 2.\n",
    "\n",
    "        --> In general, the covariates in LC choice models are often found to only weakly explain the class membership. However, even if the covariates are not statistically significant, the LC model still provides key behavioural insights into the taste heterogeneity in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 1:  Testing covariates `<br>\n",
    "\n",
    "`A.` Re-estimate the 2-class LC model using gender (i.e. sex) as a covariate<br>\n",
    "`B.` Re-estimate the 2-class LC model using education as a covariate, but considering education as a binary variable: {edu<=4: \"low\", edu> 4: \"high\"}<br>\n",
    "Hint: To do so, you can simply add the logical operator inside the PanelLikelihoodTrajectory(edu<=4)**(1/T) term.<br>\n",
    "`C.` Compare the results (fit and estimates) of the three models with covariates (i.e. edu, sex, edu<=4). Which model is better? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `5. Exploring robustness and convergence stability` <br>\n",
    "The log-likelihood function is a mathematical function that represents the log of the probability of observing the given data under a specific statistical model, as a function of the model parameters. It is used to estimate the parameters of the choice model that are most likely to have produced the observed data.\n",
    "<br><br>\n",
    "However, when estimation a choice model, the estimation algorithm may get stuck in **local** maxima. This happens if the likelihood function of the model is not **globally concave**. Local maxima are solutions where the function reaches a high value but not the highest possible. In contrast, a global maximum refers to the highest value of the function across its entire domain. In case, an optimisation algorithm gets stuck in a local solution, it does not recover the true maximum likelihood estimates. Such estimates are thus biased are should never be used for policy making. Hence, choice modellers should be aware of the possibility that optimisation algorithms can get stuck in local solutions. Unfortunately, getting stuck in local solutions is a common issue with the estimation of LC choice models. \n",
    "<br><br>\n",
    "One strategy to deal with local maxima is to try different starting points. Below, we estimate the 2-class LC model using 20 different starting points. Specifically, we use a random stanrting point between -1 and 0 for all parameters, using the rand() function. Furthermore, we set the seed number for each ran to a different value, to ensure that the starting points are different (and reproducible). One can never be sure that the estimated parameters are the global maximum, but by trying different starting points, one can increase the likelihood of finding the global maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store the LL, run, and results object\n",
    "LL = []\n",
    "run = []\n",
    "lst_LC_results = []\n",
    "\n",
    "# Set the number of runs\n",
    "N_runs = 25\n",
    "print(f\"Starting estimation of {N_runs} runs ...\")\n",
    "for i in range(1,N_runs+1):\n",
    "    \n",
    "    # Set the seed for the random number generator\n",
    "    random.seed(i)\n",
    "\n",
    "    # Give a name to the model    \n",
    "    model_name = 'LC with 2 classes, run ' + str(i)\n",
    "\n",
    "    # Define the model parameters for class 0\n",
    "    B_tt_0    = Beta('B_tt_0',  -rand(), None, None, 0)\n",
    "    B_cong_0  = Beta('B_cong_0',-rand(), None, None, 0)\n",
    "    B_var_0   = Beta('B_var_0', -rand(), None, None, 0)\n",
    "    B_tc_0    = Beta('B_tc_0',  -rand(), None, None, 0)\n",
    "\n",
    "    # Define the utility functions for class 1\n",
    "    B_tt_1    = Beta('B_tt_1',  -rand(), None, None, 0)\n",
    "    B_cong_1  = Beta('B_cong_1',-rand(), None, None, 0)\n",
    "    B_var_1   = Beta('B_var_1', -rand(), None, None, 0)\n",
    "    B_tc_1    = Beta('B_tc_1',  -rand(), None, None, 0)\n",
    "\n",
    "    # Define the membership model parameters\n",
    "    delta_0 = Beta('delta_0',  0,       None, None, 1)\n",
    "    delta_1 = Beta('delta_1',  -rand(), None, None, 0)\n",
    "    \n",
    "    # Define the utility functions for class 0\n",
    "    V1_0 = B_tt_0 * TT1 + B_cong_0 * CONG1 + B_var_0 * VAR1 + B_tc_0 * TC1\n",
    "    V2_0 = B_tt_0 * TT2 + B_cong_0 * CONG2 + B_var_0 * VAR2 + B_tc_0 * TC2\n",
    "    V3_0 = B_tt_0 * TT3 + B_cong_0 * CONG3 + B_var_0 * VAR3 + B_tc_0 * TC3\n",
    "\n",
    "    # Define the utility functions for class 1\n",
    "    V1_1 = B_tt_1 * TT1 + B_cong_1 * CONG1 + B_var_1 * VAR1 + B_tc_1 * TC1\n",
    "    V2_1 = B_tt_1 * TT2 + B_cong_1 * CONG2 + B_var_1 * VAR2 + B_tc_1 * TC2\n",
    "    V3_1 = B_tt_1 * TT3 + B_cong_1 * CONG3 + B_var_1 * VAR3 + B_tc_1 * TC3\n",
    "\n",
    "    # Create a dictionary associating utility functions with the numbering of alternatives in the \"choice\" column\n",
    "    V_0 = {1: V1_0, 2: V2_0, 3: V3_0}\n",
    "    V_1 = {1: V1_1, 2: V2_1, 3: V3_1}\n",
    "\n",
    "    # Put the dictionaries of utility functions in a list\n",
    "    V = [V_0, V_1]\n",
    "\n",
    "    # Create a dictionary to describe the availability conditions of each alternative, where 1 indicates that the alternative is available, and 0 indicates that the alternative is not available.\n",
    "    AV = {1: AV1, 2: AV2, 3: AV3} \n",
    "\n",
    "    # Define the membership model value functions for each class\n",
    "    nu_0 = delta_0 # Note: one class needs to be fixed to 0. delta_0 is fixed to 0\n",
    "    nu_1 = delta_1\n",
    "\n",
    "    # Put membership functions in a list\n",
    "    nu = [nu_0, nu_1]\n",
    "\n",
    "    # Estimate the LC model\n",
    "    results_LC = estimate_LC(V,AV,nu,CHOICE,biodata_panel,model_name)\n",
    "\n",
    "    # Get the log-likelihood value  \n",
    "    ll = results_LC.get_general_statistics()['Final log likelihood'][0]\n",
    "\n",
    "    # Store the BIC and LL values in lists\n",
    "    LL.append(ll)\n",
    "    run.append(i)\n",
    "    lst_LC_results.append(results_LC)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'Run {i:2d}/{N_runs}\\t LL = {np.round(ll,3)}')\n",
    "\n",
    "df_runs = pd.DataFrame({'run': run, 'LL':LL}).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the log-likelihood values\n",
    "df_runs.hist(column='LL', bins=10, edgecolor='black', linewidth=1)\n",
    "plt.xlabel('Log-likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of log-likelihood values for 20 runs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**<br>\n",
    "\n",
    "        --> The histogram shows that a substantial share of the estimation end up in a local maxima. The best known solution (and probably the global maximum) is the model with LL = -2413.167\n",
    "        \n",
    "        --> The implication of the existance of local maxima is that we need to estimate the model multiple times with different starting points to increase the likelihood of finding the global maximum.\n",
    "\n",
    "        --> Using good starting points is also helpful to avoid local maxima. In this case, we used negative starting points because we expect all marginal utilities (betas) to be negative. However, the starting points were probably too large (-1,0) as the betas in this model were found to be considerably smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 2:  Investigating parameter stability `<br>\n",
    "\n",
    "`A.` Investigate the parameter stability. To do so, <br>\n",
    "1. Put all the results in a dataframe, using `df_beta_values = pd.DataFrame([lst_LC_results[i].get_beta_values() for i in range(N_runs)])` <br>\n",
    "2. Show the results in a boxplot using `sns.boxplot(data=df_beta_values.iloc[:,:-1], showfliers=False)`. Try turning on and off the showfliers option to see the full distribution of the estimates. Are the estimates stable across the different runs? <br>\n",
    "\n",
    "`B.` Investigate the parameter stability of the \"good runs\". To do so,  <br>\n",
    "1. Add the Log-likelihoods to the dataframe, using `df_beta_values_LL = pd.concat((df_runs,df_beta_values), axis = 1)`<br>\n",
    "2. Keep only the results with the LL of the best known solution (with a small margin), using `df_beta_values_opt = df_beta_values_LL[df_beta_values_LL['LL'] > -2414]`<br>\n",
    "3. Carefully inspect df_beta_values_opt. Do you see a symmetry in the estimations? Explain why this symmetry occurs. <br>\n",
    "4. Are the estimates stable across these runs? <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEN1721",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
